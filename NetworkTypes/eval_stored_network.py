from dummy_datagenerator import generate_one_sample, plot_6_images, eval_output
import numpy as np
import keras
import keras.models
import tensorflow.keras.models
import matplotlib.pyplot as plt


def load_model(name, verbose=False):
    model = None
    if verbose:
        print("collecting MODEL :",name)
    try:
        model = keras.models.load_model(name)
    except:
        if verbose:
            print("no Keras Model, trying tf")
    if model is None:
        try:
            model = tensorflow.keras.models.load_model(name)
        except:
            if verbose:
                print("no tf model, collecting failed")
            return None
    return model

def collect_all(prefix, postfix, max_idx, minidx=1, n_inputs=5, diffToLabel=2, channelsLast=True):
    assert isinstance(prefix, str) and isinstance(postfix, str)
    name = prefix+"{}"+postfix+".h5"
    np.random.seed(13370)
    data, label = generate_one_sample((100, 100), n_inputs, schrittweite=10, pad=diffToLabel, channelsLast=channelsLast)
    loss = []
    for i in range(minidx, max_idx+1):
        model = load_model(name.format(i), verbose=True)
        prediction = model.predict(np.expand_dims(data, axis=0))
        MSE = (np.square(label.flatten() - prediction)).mean(axis=1)
        loss.append((MSE))
    return loss, prediction, data, label

def tmp():
    #Net_with_BNorm\Test_UPsampling 0 bis 100:
    a = [0.07082027, 0.07071756, 0.07017423, 0.06912825, 0.06735127, 0.06343446, 0.06108594, 0.05746083, 0.05178675, 0.05035387, 0.04835293, 0.04970635, 0.05103496, 0.05095759, 0.05099259, 0.04965151, 0.05111886, 0.0507963, 0.05178692, 0.05118482, 0.04974383, 0.04848581, 0.04736132, 0.04908592, 0.048663, 0.04708034, 0.04431443, 0.04411153, 0.04360704, 0.04342454, 0.04650054, 0.04334002, 0.04285875, 0.04193719, 0.04009727, 0.04112251, 0.04069725, 0.03910555, 0.03850844, 0.0353994, 0.03521872, 0.03509993, 0.03292776, 0.03244655, 0.03139779, 0.03245993, 0.03278477, 0.02892794, 0.02923199, 0.02855867, 0.02787102, 0.02816582, 0.02801414, 0.02757949, 0.02983833, 0.03256685, 0.03116961, 0.02922788, 0.02882968, 0.0274506, 0.02940817, 0.02979621, 0.02745771, 0.0297708, 0.02669674, 0.02794429, 0.03084954, 0.02739279, 0.02741094, 0.03198887, 0.02680017, 0.03098881, 0.03377213, 0.02585947, 0.02678964, 0.02798154, 0.02739216, 0.02596252, 0.02519062, 0.02671456, 0.02593215, 0.02593467, 0.02561906, 0.02734603, 0.02665286, 0.02645992, 0.02586793, 0.02584013, 0.0285903, 0.02811368, 0.02482824, 0.02627257, 0.02907563, 0.02985584, 0.03140557, 0.03005476, 0.03070098, 0.02799104, 0.02611267, 0.02797946]
    #Net withhout BNorm
    b = [0.03712038, 0.03706831, 0.03283995, 0.03315947, 0.03247645, 0.03262772, 0.03234086, 0.03215834, 0.03182612, 0.03158877, 0.03133161, 0.03125492, 0.03089586, 0.03083892, 0.03062421, 0.03046227, 0.03050168, 0.03022401, 0.03039392, 0.03000144, 0.02957207, 0.02963388, 0.02929877, 0.02915087, 0.02910803, 0.02895042, 0.02899164, 0.02871222, 0.02884152, 0.02831759, 0.02846557, 0.02800551, 0.02837778, 0.02803842, 0.02884639, 0.02790865, 0.02781529, 0.027656, 0.02787955, 0.02769676, 0.0277302, 0.02764059, 0.02808214, 0.02740386, 0.02727345, 0.02776105, 0.02762017, 0.02738418, 0.02729414, 0.02732294, 0.02695567, 0.02690032, 0.02708823, 0.02711621, 0.0271256, 0.02814355, 0.02719851, 0.02702138, 0.02712882, 0.0270189, 0.026787, 0.02721783, 0.02671615, 0.02687389, 0.02742819, 0.02676965, 0.02674696, 0.02760559, 0.02661898, 0.02659569, 0.02677082, 0.02703062, 0.02667039, 0.02658824, 0.02674637, 0.02667281, 0.0269501, 0.02669678, 0.02657939, 0.02630661, 0.0266343, 0.02674792, 0.02655195, 0.02705544, 0.0268317, 0.02704188, 0.02643453, 0.02625109, 0.02613913, 0.02646463, 0.02642541, 0.02645974, 0.02640416, 0.02642055, 0.02699639, 0.02611283, 0.02600336, 0.02598813, 0.02612279, 0.02615611]
    #2Layer Unet
    c = [0.07087346, 0.07087346, 0.07087346, 0.07084402, 0.07045755, 0.06830267, 0.06400828, 0.05907896, 0.05375447, 0.04870802, 0.04498475, 0.04254185, 0.04084118, 0.04018543, 0.04019042, 0.04081501, 0.04124546, 0.04167619, 0.04172014, 0.04236753, 0.04210292, 0.0418022, 0.04217361, 0.04269338, 0.04306339, 0.04337238, 0.04357881, 0.0442768, 0.04479199, 0.04474414, 0.04580986, 0.04551309, 0.04511041, 0.04528466, 0.04568016, 0.04491539, 0.04394656, 0.04343481, 0.04386788, 0.04259012, 0.04242064, 0.04106929, 0.0399488, 0.03846292, 0.03761783, 0.03572772, 0.0348299, 0.03317945, 0.03518129, 0.03103931, 0.03037016, 0.02907625, 0.02965791, 0.02862004, 0.02836551, 0.02835252, 0.027924, 0.02496593, 0.0231181, 0.0227569, 0.02037017, 0.01978297, 0.01841197, 0.01716827, 0.0180898, 0.01718159, 0.01563794, 0.01596008, 0.01902544, 0.01886801, 0.01676568, 0.01793253, 0.01557857, 0.01523637, 0.01517081, 0.01617425, 0.01623069, 0.01344122, 0.01305112, 0.01234665, 0.01257212, 0.01216449, 0.01298738, 0.01233167, 0.01405924, 0.0142876, 0.01294363, 0.01225939, 0.01165134, 0.01136607, 0.01149881, 0.01255034, 0.01360717, 0.01461295, 0.01681049, 0.0200147, 0.01817852, 0.01737751, 0.01295479, 0.01171606]

    plt.plot(a, label="Upsampling with BNorm")
    plt.plot(b, label="Upsampling without BNorm")
    plt.plot(c, label="tiny UNet")
    plt.legend()
    plt.ylabel('loss')
    plt.xlabel("epoch")
    plt.show()
    return

if __name__ == '__main__':

    #tmp()
    #input("Finished")
    loss1, pred1, data1, label1 = collect_all("Training_De\\CNN_deeper_", "epoch_25k", 20, 1)
    print("\n###########################\nAb hier bitte kopieren:\n")
    print("l1",loss1)

### Plotten
    plt.plot(loss1)
    plt.ylabel('loss')
    plt.xlabel("epoch")
    plt.title("2 Layer UNet")
    plt.show()

